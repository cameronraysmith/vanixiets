# Pragmatics

The choice of approach within the hierarchy is determined by the intersection of three factors: the scientific question being asked, the available data, and the computational budget.
This section provides guidance on navigating these tradeoffs within the framework's philosophical commitments.

## Matching approach to question

When the question is *mechanistic* ("what process generates this phenomenon?", "what is the causal structure of this regulatory network?", "why does this system exhibit bistability?"), the framework demands approaches from Tiers 1-2.
Derived effective theories are preferred when the fine-grained description is known and the coarse-graining is tractable.
Mechanistic Bayesian inference is preferred when the generative model can be simulated but the coarse-graining is not analytically tractable.

When the question is *structural* ("what parameters govern this process?", "which interactions are present?", "how does uncertainty propagate through this system?"), Tier 2-3 approaches are appropriate.
The full diagnostic workflow applies, and the distinction between mechanistic and phenomenological models matters less than the quality of the uncertainty quantification.

When the question is *predictive* ("what will happen under these conditions?", "which genes will be differentially expressed?"), phenomenological and even pattern-recognition approaches may be adequate — provided the prediction is within the training distribution and the uncertainty is honestly quantified.
Predictive success does not automatically translate to mechanistic understanding, and the framework requires that this distinction be maintained explicitly.
A model may predict well and explain nothing.

When the question is *exploratory* ("what structure exists in this data?", "what regularities might suggest mechanistic hypotheses?"), Tier 3-5 approaches serve the abductive stage of inquiry.
The commitment is that any hypothesis generated by exploration must subsequently be formulated as a mechanistic model and tested with severity before it counts as scientific evidence.

## Data regime considerations

When data are *scarce or expensive*, the prior becomes critical.
The iterative workflow's emphasis on prior predictive checks and domain expertise elicitation is most valuable in this regime.
Hierarchical models with informative priors allow borrowing strength across related units (genes, cells, experimental conditions) while encoding structural knowledge.
Amortized inference is less useful here because training the inference network requires many simulated datasets, which is only justified if the fitted model will be applied to many observed datasets.

When data are *abundant but noisy*, the model's ability to account for noise-generating processes becomes critical.
Mechanistic models that explicitly represent measurement processes, technical variation, and biological heterogeneity have an advantage over models that treat these as unstructured residual variance.
The aspirational context should enumerate noise sources, and the iterative expansion should prioritize incorporating those with the largest expected impact on inferences.

When data are *high-dimensional* (thousands of genes, millions of cells), computational scalability constrains the choice of inference method.
Amortized simulation-based inference provides a principled tradeoff: invest computation once (training the inference network on simulated data from the mechanistic model) and amortize it across all subsequent observations.
The key is that the simulated training data come from the mechanistic model, so the amortization preserves the model's interpretive content.
The diagnostic workflow (simulation-based calibration, coverage checks) verifies that the amortization introduces acceptable approximation error.

## Computational budget allocation

The framework implies a specific priority ordering for computational investment:

First, invest in *model construction and criticism* — the iterative cycle of Stages 1-7.
This is the intellectually productive stage where scientific understanding advances.
Computational cost here is well spent because each iteration either confirms adequacy or identifies specific inadequacies that guide improvement.

Second, invest in *inference quality* — ensuring that the posterior for the current model is accurately computed.
Unreliable inference contaminates all downstream stages: posterior predictive checks become meaningless if the posterior itself is wrong.
Simulation-based calibration is the diagnostic that gates everything else.

Third, invest in *computational acceleration* — amortized inference, surrogate models, GPU-accelerated simulation — only after the model and inference quality are established.
Accelerating inference on a misspecified model or with a biased algorithm produces wrong answers faster.

## Navigating poorly understood domains

When the domain is poorly understood and even the symmetries are uncertain, the methodology begins at Stage 1 (aspirational context) with an emphasis on exploration.
Use flexible models to identify regularities, then progressively replace flexible components with mechanistic ones as understanding develops.

The iterative cycle applies at the meta-level: the choice of model class is itself subject to the expand-and-test cycle.
An early iteration might use a Gaussian process to identify nonlinear structure in a regulatory relationship; a later iteration replaces the GP with a Hill function derived from binding kinetics; a still later iteration derives the Hill function's parameters from a master equation for promoter switching.
Each replacement is a move from a less constrained to a more constrained model — from higher to lower tier number — as understanding accumulates.

This progression is not mandated in every project.
Some questions are best answered at Tier 3 and need not be pushed to Tier 1.
The framework's commitment is not that all inquiry must reach the derived effective theory level, but that the *evidential claims* made at each tier are calibrated to the tier's capacity.
A phenomenological model supports claims about statistical structure; a mechanistic model supports claims about mechanism; a derived effective theory supports claims about the relationship between scales.
Overclaiming — asserting mechanistic conclusions from phenomenological models, or causal conclusions from pattern recognition — is precisely the pathology the framework is designed to prevent.

## The self-correcting principle

The overarching pragmatic principle, which is ultimately Peircean, is that the method of inquiry must be self-correcting.
No single modeling choice is permanent.
What matters is that each choice is testable, that the tests are severe, and that failures of testing drive principled expansion rather than ad hoc patching.

This applies not only to individual models within a project but to the methodology itself.
If a modeling approach consistently fails to produce adequate models in a given domain, that is evidence that the approach's assumptions are violated in that domain — a meta-level observation that should drive methodological refinement.
The framework is not dogma; it is itself an effective theory of how to do science, subject to the same iterative refinement it prescribes for scientific models.
